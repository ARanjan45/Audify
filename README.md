ğŸ§ Audify â€” AI-Powered Environmental Sound Classification

Audify is an end-to-end deep learning system for classifying real-world environmental sounds using convolutional neural networks (CNNs). The project demonstrates a complete ML workflow â€” from raw audio preprocessing and feature extraction to model inference, interpretability, and deployment.

<p align="center"> <img src="assets/audify-upload.png" width="800"/> </p>
ğŸš€ Overview

Environmental sound understanding is a key problem in audio intelligence. Audify solves this by converting raw audio signals into log Mel-spectrograms and using a CNN to learn meaningful audio representations for accurate sound classification.

The system is designed for real-time inference, model transparency, and scalability, making it suitable for practical AI applications.

âœ¨ Features

ğŸµ Classification of 50 environmental sound classes

ğŸ§  CNN-based deep learning model

ğŸ”Š Log Mel-spectrogram feature extraction

ğŸ§ª Data augmentation (SpecAugment)

âš¡ Real-time audio inference

ğŸ“Š Top-K predictions with confidence scores

ğŸ” CNN feature map visualization for model interpretability

ğŸŒ Scalable deployment using cloud GPU infrastructure

ğŸ“Š Classification Results

Audify outputs ranked predictions along with confidence scores for each detected sound class.

<p align="center"> <img src="assets/classification-results.png" width="800"/> </p>
ğŸ”Š Audio Representation

Raw audio waveforms are converted into log Mel-spectrograms before being passed to the neural network. Both the waveform and spectrogram are visualized to provide insight into the input representation.

<p align="center"> <img src="assets/spectrogram-waveform.png" width="800"/> </p>
ğŸ§  Model Interpretability â€” Feature Map Visualization

To improve transparency, Audify visualizes intermediate CNN feature maps. This allows users to observe how convolutional layers progressively extract higher-level audio features from raw spectrograms.

<p align="center"> <img src="assets/feature-maps.png" width="900"/> </p>
ğŸ—ï¸ Model Architecture

Convolutional Neural Network (CNN)

Stacked convolution + ReLU blocks

Batch normalization and dropout for regularization

Adaptive average pooling

Fully connected classification head

The model is trained on log Mel-spectrograms derived from raw audio signals.

ğŸ—‚ï¸ Dataset

ESC-50 Dataset

2,000 labeled environmental audio clips

50 classes Ã— 40 clips per class

Each clip is 5 seconds long

Balanced dataset suitable for benchmarking

ğŸ‹ï¸ Training Details

Framework: PyTorch

Audio Processing: Torchaudio

Loss Function: Cross-Entropy Loss

Optimizer: Adam

Batch Size: 32

Augmentation Techniques:

Time Masking

Frequency Masking

ğŸŒ Deployment

Audify is deployed as a scalable inference service using Modal, enabling GPU-accelerated audio classification through an API interface.

Inference pipeline:

Audio upload

Preprocessing and normalization

Feature extraction

CNN inference

Top-K prediction output

ğŸ“¦ Tech Stack

Language: Python

Deep Learning: PyTorch

Audio Processing: Torchaudio, Librosa

Model Type: Convolutional Neural Networks (CNNs)

Deployment: Modal

Visualization: Matplotlib

ğŸ¯ Use Cases

Environmental monitoring systems

Smart surveillance applications

Audio-based event detection

Assistive technologies

AI/ML research and experimentation

ğŸ“ˆ Future Enhancements

Multi-label sound classification

Streaming audio support

Attention-based architectures

Explainability techniques (Grad-CAM for audio)

Mobile-friendly inference endpoints

ğŸ‘¤ Author

Aprajita Ranjan

GitHub: https://github.com/ARanjan45

LinkedIn: https://www.linkedin.com/in/aprajita-ranjan-961a0523b

â­ Why Audify?

Audify showcases strong fundamentals in audio signal processing, deep learning, model interpretability, and production deployment, making it a solid project for AI/ML internships and research roles.